"""
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gH-f-ghUGn7CcUzAxskGr9CNjh3BTrty

# Лабораторная работа 5. Обучение и тестирование модели.

- Необходимо загрузить исходный набор данных и соответствующие метки классов.
- Произвести разделение загруженного набора данных на обучающую, тестовую и валидационую выборки
    (в соотношении 80:10:10). Проверить, что сформированные выборки сбалансированы.
- Написать модель нейронной сети для решения задачи прогнозирования временных рядов.
- Описать пайплайн предобработки данных и конструирования признаков.
- Написать train loop (цикл обучения). Провести эксперименты по обучению с различными значениями параметров
    learning rate (скорость обучения) и batch size (размер мини-пакета). Выбрать по 3 значения для learning rate
    и batch size (итоговое количество экспериментов будет 9).
- Для каждого проведенного эксперимента вывести графики для значения функции потерь
    (ось x - итерация обучения/номер эпохи; ось y - значение функции потерь) и выбранной метрики качества
    (ось x - итерация обучения/номер эпохи; ось y - значение метрики качества).
    Графики необходимо выводить как для обучающей, так и для валидационной выборки.
- Оценить качество работы модели на тестовой выборке.
- Сделайте выводы по полученным результатам проведенных экспериментов.
    Какую модель из всех полученных стоит использовать?
- Сохранить обученную модель.
- Выполните повторную инициализацию модели и загрузку весов. Продемонстрируйте работоспособность модели.
- Выведите предсказанные и истинные значения для заданного месяца.
"""

import io

from datetime import datetime

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

import torch
from torch.utils.data import TensorDataset, DataLoader
import torch.nn as nn
import torch.optim as optim

import plotly.graph_objs as go
from plotly.offline import iplot

from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

from google.colab import files


def plot_dataset(_df, title):
    """
    Plot dataset
    Args:
        _df:
        title:
    """
    data = []

    value = go.Scatter(
        x=_df.index,
        y=_df["Temperature"]
    )
    data.append(value)

    layout = dict(
        title=title,
        xaxis=dict(title="Date", ticklen=5, zeroline=False),
        yaxis=dict(title="Temperature", ticklen=5, zeroline=False),
    )

    fig = dict(data=data, layout=layout)
    iplot(fig)


def generate_time_lags(_df, n_lags):
    """
    Generate time lags
    Args:
        _df:
        n_lags:
    """
    df_n = _df.copy()
    for n in range(1, n_lags + 1):
        df_n[f"lag{n}"] = df_n["Temperature"].shift(n)
    df_n = df_n.iloc[n_lags:]
    return df_n


def feature_label_split(_df, target_col):
    """
    Split dataset to features and labels
    Args:
        _df:
        target_col:
    """
    y = _df[[target_col]]
    x = _df.drop(columns=[target_col])
    return x, y


def train_val_test_split(_df, target_col, test_ratio):
    """
    Split dataset to train, validation and test
    Args:
        _df:
        target_col:
        test_ratio:
    """
    val_ratio = test_ratio / (1 - test_ratio)
    X, y = feature_label_split(_df, target_col)
    x_train, _x_test, _y_train, _y_test = train_test_split(X, y, test_size=test_ratio, shuffle=False)
    x_train, _x_val, _y_train, _y_val = train_test_split(x_train, _y_train, test_size=val_ratio, shuffle=False)
    return x_train, _x_val, _x_test, _y_train, _y_val, _y_test


def get_scaler(_scaler: str):
    """
    Get scaler
    Args:
        _scaler:
    """
    scalers = {
        "minmax": MinMaxScaler,
        "standard": StandardScaler,
        "maxabs": MaxAbsScaler,
        "robust": RobustScaler,
    }
    return scalers.get(_scaler.lower())()


class RNNModel(nn.Module):
    def __init__(self, _input_dim, _hidden_dim, _layer_dim, _output_dim, dropout_prob):
        """The __init__ method that initiates an RNN instance.

        Args:
            _input_dim (int): The number of nodes in the input layer
            _hidden_dim (int): The number of nodes in each layer
            _layer_dim (int): The number of layers in the network
            _output_dim (int): The number of nodes in the output layer
            dropout_prob (float): The probability of nodes being dropped out

        """
        super(RNNModel, self).__init__()

        # Defining the number of layers and the nodes in each layer
        self.hidden_dim = _hidden_dim
        self.layer_dim = _layer_dim

        # RNN layers
        self.rnn = nn.RNN(
            _input_dim, _hidden_dim, _layer_dim, batch_first=True, dropout=dropout_prob
        )
        # Fully connected layer
        self.fc = nn.Linear(_hidden_dim, _output_dim)

    def forward(self, x):
        """The forward method takes input tensor x and does forward propagation

        Args:
            x (torch.Tensor): The input tensor of the shape (batch size, sequence length, input_dim)

        Returns:
            torch.Tensor: The output tensor of the shape (batch size, output_dim)

        """
        # Initializing hidden state for first input with zeros
        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()

        # Forward propagation by passing in the input and hidden state into the model
        out, h0 = self.rnn(x, h0.detach())

        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)
        # so that it can fit into the fully connected layer
        out = out[:, -1, :]

        # Convert the final state to our desired output shape (batch_size, output_dim)
        out = self.fc(out)
        return out


class Optimization:
    """Optimization is a helper class that allows training, validation, prediction.

    Optimization is a helper class that takes model, loss function, optimizer function
    learning scheduler (optional), early stopping (optional) as inputs. In return, it
    provides a framework to train and validate the models, and to predict future values
    based on the models.

    Attributes:
        model (RNNModel, LSTMModel, GRUModel): Model class created for the type of RNN
        loss_fn (torch.nn.modules.Loss): Loss function to calculate the losses
        optimizer (torch.optim.Optimizer): Optimizer function to optimize the loss function
        train_losses (list[float]): The loss values from the training
        val_losses (list[float]): The loss values from the validation
        last_epoch (int): The number of epochs that the models is trained
    """

    def __init__(self, _model, _loss_fn, _optimizer):
        """
        Args:
            _model (RNNModel, LSTMModel, GRUModel): Model class created for the type of RNN
            _loss_fn (torch.nn.modules.Loss): Loss function to calculate the losses
            _optimizer (torch.optim.Optimizer): Optimizer function to optimize the loss function
        """
        self.model = _model.to(device)
        self.loss_fn = _loss_fn
        self.optimizer = _optimizer
        self.train_losses = []
        self.val_losses = []

    def train_step(self, x, y):
        """The method train_step completes one step of training.

        Given the features (x) and the target values (y) tensors, the method completes
        one step of the training. First, it activates the train mode to enable back prop.
        After generating predicted values (yhat) by doing forward propagation, it calculates
        the losses by using the loss function. Then, it computes the gradients by doing
        back propagation and updates the weights by calling step() function.

        Args:
            x (torch.Tensor): Tensor for features to train one step
            y (torch.Tensor): Tensor for target values to calculate losses

        """
        # Sets model to train mode
        self.model.train()

        # Makes predictions
        yhat = self.model(x)

        # Computes loss
        loss = self.loss_fn(y, yhat)

        # Computes gradients
        loss.backward()

        # Updates parameters and zeroes gradients
        self.optimizer.step()
        self.optimizer.zero_grad()

        # Returns the loss
        return loss.item()

    def train(self, _train_loader, _val_loader, batch_size=64, n_epochs=50, n_features=1):
        """The method train performs the model training

        The method takes DataLoaders for training and validation datasets, batch size for
        mini-batch training, number of epochs to train, and number of features as inputs.
        Then, it carries out the training by iteratively calling the method train_step for
        n_epochs times. If early stopping is enabled, then it  checks the stopping condition
        to decide whether the training needs to halt before n_epochs steps. Finally, it saves
        the model in a designated file path.

        Args:
            _train_loader (torch.utils.data.DataLoader): DataLoader that stores training data
            _val_loader (torch.utils.data.DataLoader): DataLoader that stores validation data
            batch_size (int): Batch size for mini-batch training
            n_epochs (int): Number of epochs, i.e., train steps, to train
            n_features (int): Number of feature columns

        """
        model_path = f'{self.model}_{datetime.now().strftime("%Y-%m-%d %H:%M:%S")}'

        for epoch in range(1, n_epochs + 1):
            batch_losses = []
            for x_batch, y_batch in _train_loader:
                x_batch = x_batch.view([batch_size, -1, n_features]).to(device)
                y_batch = y_batch.to(device)
                loss = self.train_step(x_batch, y_batch)
                batch_losses.append(loss)
            training_loss = np.mean(batch_losses)
            self.train_losses.append(training_loss)

            with torch.no_grad():
                batch_val_losses = []
                for x_val, _y_val in _val_loader:
                    x_val = x_val.view([batch_size, -1, n_features]).to(device)
                    _y_val = _y_val.to(device)
                    self.model.eval()
                    yhat = self.model(x_val)
                    val_loss = self.loss_fn(_y_val, yhat).item()
                    batch_val_losses.append(val_loss)
                validation_loss = np.mean(batch_val_losses)
                self.val_losses.append(validation_loss)

            if (epoch <= 10) | (epoch % 50 == 0):
                print(
                    f"[{epoch}/{n_epochs}] Training loss: {training_loss:.4f}\t Validation loss: {validation_loss:.4f}"
                )

        torch.save(self.model.state_dict(), model_path)

    def evaluate(self, _test_loader, batch_size=1, n_features=1):
        """The method evaluate performs the model evaluation

        The method takes DataLoaders for the test dataset, batch size for mini-batch testing,
        and number of features as inputs. Similar to the model validation, it iteratively
        predicts the target values and calculates losses. Then, it returns two lists that
        hold the predictions and the actual values.

        Note:
            This method assumes that the prediction from the previous step is available at
            the time of the prediction, and only does one-step prediction into the future.

        Args:
            _test_loader (torch.utils.data.DataLoader): DataLoader that stores test data
            batch_size (int): Batch size for mini-batch training
            n_features (int): Number of feature columns

        Returns:
            list[float]: The values predicted by the model
            list[float]: The actual values in the test set.

        """
        with torch.no_grad():
            _predictions = []
            _values = []
            for x_test, _y_test in _test_loader:
                x_test = x_test.view([batch_size, -1, n_features]).to(device)
                _y_test = _y_test.to(device)
                self.model.eval()
                yhat = self.model(x_test)
                _predictions.append(yhat.to(device).detach().numpy())
                _values.append(_y_test.to(device).detach().numpy())

        return _predictions, _values

    def plot_losses(self):
        """The method plots the calculated loss values for training and validation
        """
        plt.plot(self.train_losses, label="Training loss")
        plt.plot(self.val_losses, label="Validation loss")
        plt.legend()
        plt.title("Losses")
        plt.show()
        plt.close()


def inverse_transform(_scaler, _df, columns):
    """
    The function inverse_transform transforms the scaled values back to the original
    Args:
        _scaler:
        _df:
        columns:
    """
    for col in columns:
        _df[col] = _scaler.inverse_transform(_df[col])
    return _df


def format_predictions(_predictions, _values, df_test, _scaler):
    """
    The function format_predictions formats the predictions and the actual values
    Args:
        _predictions:
        _values:
        df_test:
        _scaler:
    """
    vals = np.concatenate(_values, axis=0).ravel()
    preds = np.concatenate(_predictions, axis=0).ravel()
    _df_result = pd.DataFrame(data={"Temperature": vals, "prediction": preds}, index=df_test.head(len(vals)).index)
    _df_result = _df_result.sort_index()
    _df_result = inverse_transform(_scaler, _df_result, [["Temperature", "prediction"]])
    return _df_result


def calculate_metrics(_df):
    """
    The function calculate_metrics calculates the metrics for the predictions
    Args:
        _df:
    """
    _result_metrics = {'mae': mean_absolute_error(_df["Temperature"], _df.prediction),
                       'rmse': mean_squared_error(_df["Temperature"], _df.prediction) ** 0.5,
                       'r2': r2_score(_df["Temperature"], _df.prediction)}

    print("Mean Absolute Error:       ", _result_metrics["mae"])
    print("Root Mean Squared Error:   ", _result_metrics["rmse"])
    print("R^2 Score:                 ", _result_metrics["r2"])
    return _result_metrics


def build_baseline_model(_df, test_ratio, target_col):
    """
    The function build_baseline_model builds a baseline model
    Args:
        _df:
        test_ratio:
        target_col:
    """
    X, y = feature_label_split(_df, target_col)
    _x_train, _x_test, _y_train, _y_test = train_test_split(
        X, y, test_size=test_ratio, shuffle=False
    )
    _model = LinearRegression()
    _model.fit(_x_train, _y_train)
    prediction = _model.predict(_x_test)

    result = pd.DataFrame(_y_test)
    result["prediction"] = prediction
    result = result.sort_index()

    return result


if __name__ == "__main__":
    device = "cpu"
    data_to_load = files.upload()

    old_df = pd.read_csv(io.BytesIO(data_to_load['dataset.csv']))
    old_df = old_df.dropna()
    df = pd.DataFrame()
    df['DateTime'] = old_df['Date']
    df['Temperature'] = old_df['Temperature']
    print(df.columns.tolist())
    df = df.set_index('DateTime')
    print(df)

    df.index = pd.to_datetime(df.index)
    if not df.index.is_monotonic:
        df = df.sort_index()
    plot_dataset(df, title='Temprerature')

    input_dim = 100
    df_timelags = generate_time_lags(df, input_dim)
    print(df_timelags)

    X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(df_timelags, 'Temperature', 0.4)

    scaler = get_scaler('minmax')
    X_train_arr = scaler.fit_transform(X_train)
    X_val_arr = scaler.transform(X_val)
    X_test_arr = scaler.transform(X_test)

    y_train_arr = scaler.fit_transform(y_train)
    y_val_arr = scaler.transform(y_val)
    y_test_arr = scaler.transform(y_test)

    batch_size = 64

    train_features = torch.Tensor(X_train_arr)
    train_targets = torch.Tensor(y_train_arr)
    val_features = torch.Tensor(X_val_arr)
    val_targets = torch.Tensor(y_val_arr)
    test_features = torch.Tensor(X_test_arr)
    test_targets = torch.Tensor(y_test_arr)

    train = TensorDataset(train_features, train_targets)
    val = TensorDataset(val_features, val_targets)
    test = TensorDataset(test_features, test_targets)

    train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=True)
    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, drop_last=True)
    test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=True)
    test_loader_one = DataLoader(test, batch_size=1, shuffle=False, drop_last=True)

    input_dim = len(X_train.columns)
    output_dim = 1
    hidden_dim = 64
    layer_dim = 3
    batch_size = 64
    dropout = 0.2
    n_epochs = 20
    learning_rate = 1e-3
    weight_decay = 1e-6

    model_params = {'input_dim': input_dim,
                    'hidden_dim': hidden_dim,
                    'layer_dim': layer_dim,
                    'output_dim': output_dim,
                    'dropout_prob': dropout}

    model = RNNModel(**model_params)

    loss_fn = nn.MSELoss(reduction="mean")
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

    opt = Optimization(_model=model, _loss_fn=loss_fn, _optimizer=optimizer)
    opt.train(train_loader, val_loader, batch_size=batch_size, n_epochs=n_epochs, n_features=input_dim)
    opt.plot_losses()

    predictions, values = opt.evaluate(
        test_loader_one,
        batch_size=1,
        n_features=input_dim
    )

    df_result = format_predictions(predictions, values, X_test, scaler)

    result_metrics = calculate_metrics(df_result)

    df_baseline = build_baseline_model(df_timelags, 0.2, 'Temperature')
    baseline_metrics = calculate_metrics(df_baseline)
    df_result.plot()
